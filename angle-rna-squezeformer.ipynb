{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":12024591,"sourceType":"competition"},{"sourceId":10855324,"sourceType":"datasetVersion","datasetId":6742586},{"sourceId":228568247,"sourceType":"kernelVersion"},{"sourceId":233426810,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.210500Z","iopub.execute_input":"2025-05-07T18:55:46.211094Z","iopub.status.idle":"2025-05-07T18:55:46.215269Z","shell.execute_reply.started":"2025-05-07T18:55:46.211070Z","shell.execute_reply":"2025-05-07T18:55:46.214440Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/usr/lib/metric')\n\nfrom metric import score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.216513Z","iopub.execute_input":"2025-05-07T18:55:46.216731Z","iopub.status.idle":"2025-05-07T18:55:46.231582Z","shell.execute_reply.started":"2025-05-07T18:55:46.216714Z","shell.execute_reply":"2025-05-07T18:55:46.230855Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"train_sequences = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv')\n\ntrain_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_labels.csv')\n\nvalidation_sequences = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv')\n\nvalidation_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_labels.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.232361Z","iopub.execute_input":"2025-05-07T18:55:46.232606Z","iopub.status.idle":"2025-05-07T18:55:46.475199Z","shell.execute_reply.started":"2025-05-07T18:55:46.232586Z","shell.execute_reply":"2025-05-07T18:55:46.474388Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"test_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/sample_submission.csv')\ntest_labels[\"chain_name\"] = test_labels[\"ID\"].str.rsplit(\"_\", n=1).str[0]\n\n# 2. Group by chain and get sequence + length\nchain_info = (\n    test_labels\n    .groupby(\"chain_name\")[\"resname\"]\n    .agg(sequence=lambda x: ''.join(x), length='count')\n    .reset_index()\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.476835Z","iopub.execute_input":"2025-05-07T18:55:46.477047Z","iopub.status.idle":"2025-05-07T18:55:46.492288Z","shell.execute_reply.started":"2025-05-07T18:55:46.477031Z","shell.execute_reply":"2025-05-07T18:55:46.491568Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"chain_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.492892Z","iopub.execute_input":"2025-05-07T18:55:46.493073Z","iopub.status.idle":"2025-05-07T18:55:46.500615Z","shell.execute_reply.started":"2025-05-07T18:55:46.493059Z","shell.execute_reply":"2025-05-07T18:55:46.500040Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"   chain_name                                           sequence  length\n0       R1107  GGGGGCCACAGCAGAAGCGUUCACGUCGCAGCCCCUGUCAGCCAUU...      69\n1       R1108  GGGGGCCACAGCAGAAGCGUUCACGUCGCGGCCCCUGUCAGCCAUU...      69\n2       R1116  CGCCCGGAUAGCUCAGUCGGUAGAGCAGCGGCUAAAACAGCUCUGG...     157\n3     R1117v2                     UUGGGUUCCCUCACCCCAAUCAUAAAAAGG      30\n4       R1126  GGAAUCUCGCCCGAUGUUCGCAUCGGGAUUUGCAGGUCCAUGGAUU...     363\n5       R1128  GGAAUAUCGUCAUGGUGAUUCGUCACCAUGAGGCUAGAUCUCAUAU...     238\n6       R1136  GGAUACGUCUACGCUCAGUGACGGACUCUCUUCGGAGAGUCUGACA...     374\n7       R1138  GGGAGAGUACUAUUCAGAUGCAGACCGCAAGUUCAGAGCGGUUUGC...     720\n8       R1149  GGACACGAGUAACUCGUCUAUCUUCUGCAGGCUGCUUACGGUUUCG...     124\n9       R1156  GGAGCAUCGUGUCUCAAGUGCUUCACGGUCACAAUAUACCGUUUCG...     135\n10      R1189  GCGUACAGGGAACACGCAACCCCGAAGGAUCGGGGAAGGGACGUCG...     118\n11      R1190  GCGUACAGGGAACACGCAACCCCGAAGGAUCGGGGAAGGGACGUCG...     118","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chain_name</th>\n      <th>sequence</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>R1107</td>\n      <td>GGGGGCCACAGCAGAAGCGUUCACGUCGCAGCCCCUGUCAGCCAUU...</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>R1108</td>\n      <td>GGGGGCCACAGCAGAAGCGUUCACGUCGCGGCCCCUGUCAGCCAUU...</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>R1116</td>\n      <td>CGCCCGGAUAGCUCAGUCGGUAGAGCAGCGGCUAAAACAGCUCUGG...</td>\n      <td>157</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>R1117v2</td>\n      <td>UUGGGUUCCCUCACCCCAAUCAUAAAAAGG</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>R1126</td>\n      <td>GGAAUCUCGCCCGAUGUUCGCAUCGGGAUUUGCAGGUCCAUGGAUU...</td>\n      <td>363</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>R1128</td>\n      <td>GGAAUAUCGUCAUGGUGAUUCGUCACCAUGAGGCUAGAUCUCAUAU...</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>R1136</td>\n      <td>GGAUACGUCUACGCUCAGUGACGGACUCUCUUCGGAGAGUCUGACA...</td>\n      <td>374</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>R1138</td>\n      <td>GGGAGAGUACUAUUCAGAUGCAGACCGCAAGUUCAGAGCGGUUUGC...</td>\n      <td>720</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>R1149</td>\n      <td>GGACACGAGUAACUCGUCUAUCUUCUGCAGGCUGCUUACGGUUUCG...</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>R1156</td>\n      <td>GGAGCAUCGUGUCUCAAGUGCUUCACGGUCACAAUAUACCGUUUCG...</td>\n      <td>135</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>R1189</td>\n      <td>GCGUACAGGGAACACGCAACCCCGAAGGAUCGGGGAAGGGACGUCG...</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>R1190</td>\n      <td>GCGUACAGGGAACACGCAACCCCGAAGGAUCGGGGAAGGGACGUCG...</td>\n      <td>118</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"train_sequences['length'] = train_sequences['sequence'].str.len()\nvalidation_sequences['length'] = validation_sequences['sequence'].str.len()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.501259Z","iopub.execute_input":"2025-05-07T18:55:46.501424Z","iopub.status.idle":"2025-05-07T18:55:46.515138Z","shell.execute_reply.started":"2025-05-07T18:55:46.501411Z","shell.execute_reply":"2025-05-07T18:55:46.514500Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"unique_values = train_labels['resname'].value_counts()\nprint(\"Unique values:\", unique_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.515886Z","iopub.execute_input":"2025-05-07T18:55:46.516081Z","iopub.status.idle":"2025-05-07T18:55:46.534955Z","shell.execute_reply.started":"2025-05-07T18:55:46.516066Z","shell.execute_reply":"2025-05-07T18:55:46.534407Z"}},"outputs":[{"name":"stdout","text":"Unique values: resname\nG    41450\nC    33937\nA    32524\nU    29178\n-        4\nX        2\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"train_labels = train_labels.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.535685Z","iopub.execute_input":"2025-05-07T18:55:46.536396Z","iopub.status.idle":"2025-05-07T18:55:46.568844Z","shell.execute_reply.started":"2025-05-07T18:55:46.536371Z","shell.execute_reply":"2025-05-07T18:55:46.568083Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Extract protein names from 'ID' column\ntrain_labels[\"RNA_name\"] = train_labels[\"ID\"].str.rsplit(\"_\", n=1).str[0]\n\n# Count the number of amino acids per protein\nresidue_counts = train_labels[\"RNA_name\"].value_counts()\n\n# Filter proteins with fewer than 1000 amino acids\nvalid_proteins = residue_counts[residue_counts < 1000].index\n\n# Create a new dataframe with only valid proteins\nfiltered_train_labels = train_labels[train_labels[\"RNA_name\"].isin(valid_proteins)].drop(columns=[\"RNA_name\"])\n\n# Reset index\nfiltered_train_labels = filtered_train_labels.reset_index(drop=True)\n\n# Display result\nprint(filtered_train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.570653Z","iopub.execute_input":"2025-05-07T18:55:46.570834Z","iopub.status.idle":"2025-05-07T18:55:46.863448Z","shell.execute_reply.started":"2025-05-07T18:55:46.570820Z","shell.execute_reply":"2025-05-07T18:55:46.862787Z"}},"outputs":[{"name":"stdout","text":"              ID resname  resid         x_1         y_1         z_1\n0       1SCL_A_1       G      1   13.760000  -25.974001    0.102000\n1       1SCL_A_2       G      2    9.310000  -29.638000    2.669000\n2       1SCL_A_3       G      3    5.529000  -27.813000    5.878000\n3       1SCL_A_4       U      4    2.678000  -24.900999    9.793000\n4       1SCL_A_5       G      5    1.827000  -20.136000   11.793000\n...          ...     ...    ...         ...         ...         ...\n48579  8Z1F_T_62       U     62  112.516998  117.880997  119.245003\n48580  8Z1F_T_63       A     63  115.292999  116.571999  114.827003\n48581  8Z1F_T_64       C     64  115.857002  114.595001  109.509003\n48582  8Z1F_T_65       C     65  113.816002  113.236000  104.339996\n48583  8Z1F_T_66       A     66  118.279999  113.883003   98.071999\n\n[48584 rows x 6 columns]\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"class AttentionWithBias(nn.Module):\n    def __init__(self, dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n\n    def forward(self, x, attn_bias=None):\n        attn_output, _ = self.attn(x, x, x, attn_mask=attn_bias)\n        return attn_output\n\n\"\"\"\nclass Conv2DNet(nn.Module):\n    def __init__(self, input_dim, channels=1):\n        super(Conv2DNet, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, channels, kernel_size=1)  # Output shape: (batch, 1, seq_len, seq_len)\n        )\n\n    def forward(self, x):\n        # x: (batch, seq_len, embed_dim)\n        seq_len = x.size(1)\n        # Outer product: (batch, seq_len, seq_len, embed_dim)\n        outer = torch.einsum(\"bik,bjk->bij\", x, x).unsqueeze(1)\n        # outer: (batch, 1, seq_len, seq_len)\n        return self.conv(outer).squeeze(1)  # Return (batch, seq_len, seq_len)\n\"\"\"\nclass Conv2DNet(nn.Module):\n    def __init__(self, seq_len=1024, in_channels=1):\n        super(Conv2DNet, self).__init__()\n        self.seq_len = seq_len\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1)  # Output shape: (B, 1, L, L)\n        )\n\n    def forward(self, pairwise_input):\n        # pairwise_input: shape (B, 1, L, L)\n        return self.net(pairwise_input).squeeze(1)  # (B, L, L)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.864068Z","iopub.execute_input":"2025-05-07T18:55:46.864270Z","iopub.status.idle":"2025-05-07T18:55:46.880075Z","shell.execute_reply.started":"2025-05-07T18:55:46.864255Z","shell.execute_reply":"2025-05-07T18:55:46.879327Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Squeezeformer Block\nclass SqueezeformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, dropout=0.1):\n        super(SqueezeformerBlock, self).__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim)\n        )\n\n    def forward(self, x, attn_bias=None):\n        x_norm = self.norm1(x)\n        #attn_output, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=attn_bias)\n        attn_output, _ = self.attn(\n            x_norm, x_norm, x_norm,\n            attn_mask=attn_bias.repeat_interleave(self.attn.num_heads, dim=0) if attn_bias is not None else None\n        )\n\n        x = x + attn_output\n        x = x + self.ffn(self.norm2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.880871Z","iopub.execute_input":"2025-05-07T18:55:46.881082Z","iopub.status.idle":"2025-05-07T18:55:46.897107Z","shell.execute_reply.started":"2025-05-07T18:55:46.881066Z","shell.execute_reply":"2025-05-07T18:55:46.896506Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass SqueezeformerGRUModel(nn.Module):\n    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=12, dropout=0.1, max_len=1024, fixed_dist=5.65):\n        super(SqueezeformerGRUModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=vocab_size - 1)\n        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model))\n        self.fixed_dist = fixed_dist\n\n        self.squeezeformer_blocks = nn.ModuleList([\n            SqueezeformerBlock(d_model, n_heads, dropout) for _ in range(num_layers)\n        ])\n\n        self.conv2d_net = Conv2DNet(seq_len=max_len, in_channels=1)\n\n        self.gru = nn.GRU(input_size=d_model, hidden_size=d_model, num_layers=1,\n                          batch_first=True, bidirectional=True)\n\n        self.direction_head = nn.Linear(d_model * 2, 3)\n        self._initialize_direction_head()\n\n    def _initialize_direction_head(self):\n        # Initialize weights using kaiming uniform distribution\n        nn.init.kaiming_uniform_(self.direction_head.weight, a=math.sqrt(5))\n        # Initialize bias with small values\n        nn.init.normal_(self.direction_head.bias, std=0.01)\n\n    def forward(self, input_ids):\n        B, L = input_ids.shape\n        x = self.embedding(input_ids) + self.pos_embedding[:, :L, :]\n\n        x_norm = F.normalize(x, dim=-1)\n        pairwise_input = torch.matmul(x_norm, x_norm.transpose(1, 2)).unsqueeze(1)\n        attn_bias = self.conv2d_net(pairwise_input)  # (B, L, L)\n\n        for block in self.squeezeformer_blocks:\n            x = block(x, attn_bias=attn_bias)\n\n        gru_out, _ = self.gru(x)\n\n        # Compute direction vectors\n        direction_vectors = self.direction_head(gru_out[:, :-1])  # (B, L-1, 3)\n        \n        # Normalize to ensure they lie on the surface of a sphere\n        unit_vectors = F.normalize(direction_vectors, dim=-1)\n\n        # Reconstruct coordinates with fixed distance\n        coords = [torch.zeros((B, 1, 3), device=unit_vectors.device)]  # start from origin\n        for i in range(unit_vectors.shape[1]):\n            next_pos = coords[-1] + unit_vectors[:, i:i+1] * self.fixed_dist\n            coords.append(next_pos)\n\n        coords = torch.cat(coords, dim=1)  # (B, L, 3)\n        return coords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.897850Z","iopub.execute_input":"2025-05-07T18:55:46.898060Z","iopub.status.idle":"2025-05-07T18:55:46.916694Z","shell.execute_reply.started":"2025-05-07T18:55:46.898038Z","shell.execute_reply":"2025-05-07T18:55:46.916047Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"class RNATokenizer:\n    def __init__(self):\n        self.vocab = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n        self.pad_token = 4\n        self.vocab_size = len(self.vocab) + 1  # +1 for PAD token\n\n    def tokenize(self, sequence):\n        return [self.vocab.get(res, self.pad_token) for res in sequence]\n\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass RNADataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=1024):\n        self.tokenizer = tokenizer\n        self.samples = []\n        grouped = df.groupby(df['ID'].str.split('_').str[0])  # Group by RNA chain name\n\n        for name, group in grouped:\n            sequence = group['resname'].tolist()\n            coords = group[['x_1', 'y_1', 'z_1']].values.astype(np.float32)\n            coords -= coords[0]  # Shift the first residue to origin\n\n\n            if len(sequence) > max_len:\n                continue  # optionally skip long sequences\n\n            tokenized = tokenizer.tokenize(sequence)\n            # Padding\n            pad_len = max_len - len(tokenized)\n            tokenized += [tokenizer.pad_token] * pad_len\n            coords = np.pad(coords, ((0, pad_len), (0, 0)), constant_values=0)\n\n            self.samples.append((tokenized, coords))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        tokens, coords = self.samples[idx]\n        return {\n            'input_ids': torch.tensor(tokens, dtype=torch.long),\n            'target': torch.tensor(coords, dtype=torch.float32),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.917361Z","iopub.execute_input":"2025-05-07T18:55:46.917590Z","iopub.status.idle":"2025-05-07T18:55:46.929298Z","shell.execute_reply.started":"2025-05-07T18:55:46.917568Z","shell.execute_reply":"2025-05-07T18:55:46.928630Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TMLoss(nn.Module):\n    def __init__(self, eps=1e-8):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, pred, target):\n        \"\"\"\n        pred: (B, L, 3) - predicted 3D coordinates\n        target: (B, L, 3) - true 3D coordinates\n        \"\"\"\n        B, L, _ = pred.shape\n        device = pred.device\n\n        # Distance between each aligned residue pair\n        dists = torch.norm(pred - target, dim=-1)  # (B, L)\n\n        # Compute d0 based on the rules\n        d0 = torch.zeros(B, device=device)\n        for i in range(B):\n            L_ref = L  # assuming full alignment for now\n            if L_ref < 12:\n                d0[i] = 0.3\n            elif L_ref < 16:\n                d0[i] = 0.4\n            elif L_ref < 20:\n                d0[i] = 0.5\n            elif L_ref < 24:\n                d0[i] = 0.6\n            elif L_ref < 30:\n                d0[i] = 0.7\n            else:\n                d0[i] = 0.6 * ((L_ref - 0.5) ** 0.5) - 2.5\n\n        # Broadcast d0 for each residue\n        d0 = d0.view(B, 1)  # (B, 1)\n\n        tm_per_residue = 1.0 / (1.0 + (dists / (d0 + self.eps)) ** 2)  # (B, L)\n        tm_score = tm_per_residue.mean(dim=1)  # (B,) mean over residues\n\n        # We want to maximize TM-score -> so minimize (1 - TM-score)\n        loss = 1.0 - tm_score  # (B,)\n        return loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.930089Z","iopub.execute_input":"2025-05-07T18:55:46.930336Z","iopub.status.idle":"2025-05-07T18:55:46.946353Z","shell.execute_reply.started":"2025-05-07T18:55:46.930314Z","shell.execute_reply":"2025-05-07T18:55:46.945701Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nmodel = SqueezeformerGRUModel(vocab_size=5).cuda()\ninput_ids = torch.randint(0, 4, (1, 128)).cuda()\n\nwith torch.no_grad():\n    coords, = model(input_ids)\n\ndisplacements = coords[:, 1:] - coords[:, :-1]  # Shape: (B, L-1, 3)\nunit_lengths = displacements.norm(dim=-1)  # Length of each step\n\n# Diagnostic Prints\nprint(f\"Displacements shape: {displacements.shape}\")\n\n# Ensure we are not dividing by zero\neps = 1e-8\nnorms_1 = displacements[:, :-1].norm(dim=-1, keepdim=True) + eps\nnorms_2 = displacements[:, 1:].norm(dim=-1, keepdim=True) + eps\n\n# Compute cosine of angles between successive steps\nangles = (displacements[:, :-1] * displacements[:, 1:]).sum(dim=-1) / (norms_1 * norms_2)\n\nprint(angles)\n\nmean_angle = angles.mean().item()\nprint(f\"Mean cosine of angles between successive steps: {mean_angle:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T19:09:59.238471Z","iopub.execute_input":"2025-05-07T19:09:59.238751Z","iopub.status.idle":"2025-05-07T19:09:59.431467Z","shell.execute_reply.started":"2025-05-07T19:09:59.238728Z","shell.execute_reply":"2025-05-07T19:09:59.430837Z"}},"outputs":[{"name":"stdout","text":"Displacements shape: torch.Size([128, 2])\ntensor([[ 0.0000e+00, -1.4244e+16, -2.3293e+17,  ...,  3.6363e+20,\n          3.8015e+20,  3.9225e+20],\n        [ 0.0000e+00, -1.0000e+00, -1.6353e+01,  ...,  2.5529e+04,\n          2.6689e+04,  2.7539e+04],\n        [ 0.0000e+00, -6.1150e-02, -1.0000e+00,  ...,  1.5611e+03,\n          1.6320e+03,  1.6840e+03],\n        ...,\n        [ 0.0000e+00, -3.9171e-05, -6.4058e-04,  ...,  1.0000e+00,\n          1.0454e+00,  1.0787e+00],\n        [ 0.0000e+00, -3.7469e-05, -6.1274e-04,  ...,  9.5654e-01,\n          1.0000e+00,  1.0318e+00],\n        [ 0.0000e+00, -3.6312e-05, -5.9383e-04,  ...,  9.2702e-01,\n          9.6913e-01,  1.0000e+00]], device='cuda:0')\nMean cosine of angles between successive steps: 723986213131780096.0000\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.utils.data import DataLoader\n\ntokenizer = RNATokenizer()  # ✅ Define this before using it\n\n# Assuming you already defined SqueezeformerGRUModel\nmodel = SqueezeformerGRUModel(tokenizer.vocab_size)  # Define this next\nmodel = model.cuda()  # Move to GPU if available\n\ndataset = RNADataset(filtered_train_labels, tokenizer, max_len=1024)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\ncriterion =nn.MSELoss() #TMLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n\n    for batch in dataloader:\n        input_ids = batch['input_ids'].cuda()\n        target = batch['target'].cuda()\n\n        output = model(input_ids)\n        loss = criterion(output, target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    torch.save(model.state_dict(), 'best_model.pt')\n    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:55:46.946961Z","iopub.execute_input":"2025-05-07T18:55:46.947208Z","iopub.status.idle":"2025-05-07T19:00:19.950360Z","shell.execute_reply.started":"2025-05-07T18:55:46.947173Z","shell.execute_reply":"2025-05-07T19:00:19.949651Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Loss = 292005.8095\nEpoch 2: Loss = 6003.9676\nEpoch 3: Loss = 3188.3976\nEpoch 4: Loss = 3489.3704\nEpoch 5: Loss = 3736.7241\nEpoch 6: Loss = 2371.8934\nEpoch 7: Loss = 2363.9023\nEpoch 8: Loss = 2475.6477\nEpoch 9: Loss = 2532.4565\nEpoch 10: Loss = 1907.4439\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# 1. Grab and tokenize the 1SCL RNA\ndf_1SCL = train_labels[train_labels[\"ID\"].str.startswith(\"1SCL_A_\")].copy()\nseq_1SCL = ''.join(df_1SCL['resname'])  # Original sequence\ninput_ids = tokenizer.tokenize(seq_1SCL)  # Token IDs\n\ninput_tensor = torch.tensor(input_ids).unsqueeze(0).cuda()  # (1, L)\n\n# 2. Run through model\nmodel.eval()\nwith torch.no_grad():\n    pred_coords = model(input_tensor)  # (1, L, 3)\n    print(\"Predicted coords sum:\", pred_coords.abs().sum().item())\n    print(\"Output shape:\", pred_coords.shape)\npred_coords = pred_coords.squeeze(0).cpu().numpy()  # (L, 3)\n\n# 3. True coordinates\ntrue_coords = df_1SCL[['x_1', 'y_1', 'z_1']].values  # (L, 3)\n\n# 4. Plotting\nfig = plt.figure(figsize=(10, 5))\nax1 = fig.add_subplot(121, projection='3d')\nax2 = fig.add_subplot(122, projection='3d')\n\nax1.plot(true_coords[:, 0], true_coords[:, 1], true_coords[:, 2], label='True', color='green')\nax1.set_title(\"True RNA Structure\")\n\nax2.plot(pred_coords[:, 0], pred_coords[:, 1], pred_coords[:, 2], label='Predicted', color='blue')\nax2.set_title(\"Predicted RNA Structure\")\n\nfor ax in [ax1, ax2]:\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_1SCL[['x_pred','y_pred', 'z_pred']]= pred_coords\ndf_1SCL.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_1SCL = df_1SCL.copy()\ndf_1SCL['delta_x'] = df_1SCL['x_1'].diff()\ndf_1SCL['delta_y'] = df_1SCL['y_1'].diff()\ndf_1SCL['delta_z'] = df_1SCL['z_1'].diff()\ndf_1SCL.fillna(0.)\ndf_1SCL['distance'] = np.sqrt(df_1SCL['delta_x']**2 + df_1SCL['delta_y']**2 + df_1SCL['delta_z']**2)\n\ndf_1SCL['delta_x_pred'] = df_1SCL['x_pred'].diff()\ndf_1SCL['delta_y_pred'] = df_1SCL['y_pred'].diff()\ndf_1SCL['delta_z_pred'] = df_1SCL['z_pred'].diff()\ndf_1SCL.fillna(0.)\ndf_1SCL['distance_pred'] = np.sqrt(df_1SCL['delta_x_pred']**2 + df_1SCL['delta_y_pred']**2 + df_1SCL['delta_z_pred']**2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_1SCL['distance'].hist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_1SCL['distance_pred'].hist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the trained model weights before prediction\ncheckpoint_path = 'best_model.pt'  # adjust this!\nmodel.load_state_dict(torch.load(checkpoint_path))\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction on the validation set","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\nmodel.eval()\ntokenizer = RNATokenizer()\n\n# Make a copy to modify\nvalidation_predictions = validation_labels[['ID', 'resname', 'resid']]\n\n# Get all unique chain IDs (e.g., \"1SCL_A\")\nchain_ids = validation_predictions['ID'].apply(lambda x: '_'.join(x.split('_')[:1])).unique()\n\nfor chain_id in tqdm(chain_ids):\n    \n    # 1. Extract subsequence for that chain\n    chain_df = validation_predictions[validation_predictions['ID'].str.startswith(chain_id)].copy()\n    sequence = ''.join(chain_df['resname'].tolist())\n    L = len(sequence)\n\n    # 2. Tokenize\n    input_ids = tokenizer.tokenize(sequence)\n    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).cuda()  # (1, L)\n\n    # 3. Run through model\n    model.eval()\n    with torch.no_grad():\n        pred_coords = model(input_tensor)  # (1, L, 3)\n\n    # 4. Save back to DataFrame\n    pred_coords = pred_coords.squeeze(0).cpu().numpy()[:L]  # (L, 3) - clip if extra padding\n    validation_predictions.loc[validation_predictions['ID'].str.startswith(chain_id), ['x_1', 'y_1', 'z_1']] = pred_coords\n    validation_predictions.loc[validation_predictions['ID'].str.startswith(chain_id), ['x_2', 'y_2', 'z_2']] = pred_coords\n    validation_predictions.loc[validation_predictions['ID'].str.startswith(chain_id), ['x_3', 'y_3', 'z_3']] = pred_coords\n    validation_predictions.loc[validation_predictions['ID'].str.startswith(chain_id), ['x_4', 'y_4', 'z_4']] = pred_coords\n    validation_predictions.loc[validation_predictions['ID'].str.startswith(chain_id), ['x_5', 'y_5', 'z_5']] = pred_coords","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_labels.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_predictions.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tm_score = score(validation_labels, validation_predictions.copy(), row_id_column_name='ID')\nprint(f\"Average TM-score: {tm_score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/sample_submission.csv')\nsubm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\nmodel.eval()\ntokenizer = RNATokenizer()\n\n# Make a copy to modify\nsubm_predictions = subm.copy()\n\n# Get all unique chain IDs (e.g., \"1SCL_A\")\nchain_ids = subm_predictions['ID'].apply(lambda x: '_'.join(x.split('_')[:1])).unique()\n\nfor chain_id in tqdm(chain_ids):\n    \n    # 1. Extract subsequence for that chain\n    chain_df = subm_predictions[subm_predictions['ID'].str.startswith(chain_id)].copy()\n    sequence = ''.join(chain_df['resname'].tolist())\n    L = len(sequence)\n\n    # 2. Tokenize\n    input_ids = tokenizer.tokenize(sequence)\n    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).cuda()  # (1, L)\n\n    # 3. Run through model\n    model.eval()\n    with torch.no_grad():\n        pred_coords = model(input_tensor)  # (1, L, 3)\n        print(\"Chain ID:\", chain_id)\n        #print(\"Sequence:\", sequence)\n        print(\"Input tensor:\", input_tensor)\n        print(\"Predicted coords sum:\", pred_coords.abs().sum().item())\n        print(\"Output shape:\", pred_coords.shape)\n        print(\"First 5 direction vectors:\\n\", pred_coords[0, :5])\n\n    # 4. Save back to DataFrame\n    pred_coords = pred_coords.squeeze(0).cpu().numpy()[:L]  # (L, 3) - clip if extra padding\n    subm_predictions.loc[subm_predictions['ID'].str.startswith(chain_id), ['x_1', 'y_1', 'z_1']] = pred_coords","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm_predictions.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subm_predictions.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}